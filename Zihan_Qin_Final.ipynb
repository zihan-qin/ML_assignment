{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ocYLfZksvAd2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "tf.random.set_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfEZlAu7Bg8U"
   },
   "source": [
    "(a) In this problem, we are trying to build a classifier to analyze the sentiment of reviews. You are provided with text data in two folders: one folder involves positive reviews, and one folder involves negative reviews.\n",
    "\n",
    "(b) Data Exploration and Pre-processing\n",
    "\n",
    "i. You can use binary encoding for the sentiments , i.e y = 1 for positive sentiments and y = âˆ’1 for negative sentiments.\n",
    "\n",
    "\n",
    "iii. The name of each text file starts with cv number. Use text files 0-699 in each class for training and 700-999 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lNgdmV8ZtMcl"
   },
   "outputs": [],
   "source": [
    "#trainset = pd.DataFrame(columns=['content','class'])\n",
    "#testset = pd.DataFrame(columns=['content','class'])\n",
    "trainset = []\n",
    "testset = []\n",
    "path = \"../Data/neg/\"\n",
    "for i in os.listdir(path):\n",
    "  if int(i[2:5]) <= 999 and int(i[2:5]) >=700: \n",
    "    with open(path+i,'r') as f:\n",
    "      testset.append([f.read(),-1])\n",
    "#    testset = testset.append(tmp)\n",
    "  else:\n",
    "    with open(path+i,'r') as f:\n",
    "      trainset.append([f.read(),-1])\n",
    "#    trainset = trainset.append(tmp)\n",
    "path = \"../Data/pos/\"\n",
    "for i in os.listdir(path):\n",
    "  if int(i[2:5]) <= 999 and int(i[2:5]) >=700: \n",
    "    with open(path+i,'r') as f:\n",
    "      testset.append([f.read(),1])\n",
    "#    testset = testset.append(tmp)\n",
    "  else:\n",
    "    with open(path+i,'r') as f:\n",
    "      trainset.append([f.read(),1])\n",
    " #   trainset = trainset.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sZP1ImdSDPEQ"
   },
   "outputs": [],
   "source": [
    "trainset = pd.DataFrame(trainset,columns=['content','class'])\n",
    "testset = pd.DataFrame(testset,columns=['content','class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htQ1Oq8Rt83O"
   },
   "source": [
    "ii. The data are pretty clean. Remove the punctuation and numbers from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tHoqLPaQ0jlu"
   },
   "outputs": [],
   "source": [
    "def data_pre(dataset):\n",
    "  dataset.index = range(len(dataset))\n",
    "  for i in range(len(dataset)):\n",
    "    tmp = dataset.loc[i,'content']\n",
    "    tmp = tmp.translate(str.maketrans('','',string.punctuation))\n",
    "    tmp = tmp.translate(str.maketrans('','',string.digits))\n",
    "    tmp = tmp.translate(str.maketrans('\\n',' '))\n",
    "    tmp = tmp.translate(str.maketrans('\\t',' '))\n",
    "    dataset.loc[i,'content'] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6bUm7gY35iVB"
   },
   "outputs": [],
   "source": [
    "data_pre(trainset)\n",
    "data_pre(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9YkwE3HuNXQ"
   },
   "source": [
    "iv. Count the number of unique words in the whole dataset (train + test) and print it out.\n",
    "\n",
    "v. Calculate the average review length and the standard deviation of review lengths. Report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "my5jasY4tKd9",
    "outputId": "0329a566-b0f8-4529-aaed-3db39426140f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words count:46830.\n",
      "avg review length:644.36.\n",
      "std review length:284.98.\n"
     ]
    }
   ],
   "source": [
    "word_frec = {}\n",
    "review_len = []\n",
    "for i in trainset['content']:\n",
    "  tmp = i.split()\n",
    "  review_len.append(len(tmp))\n",
    "  for word in tmp:\n",
    "    word_frec[word] = word_frec.get(word,0) + 1\n",
    "for i in testset['content']:\n",
    "  tmp = i.split()\n",
    "  review_len.append(len(tmp))\n",
    "  for word in tmp:\n",
    "    word_frec[word] = word_frec.get(word,0) + 1\n",
    "print(\"unique words count:%d.\"%len(word_frec))\n",
    "print(\"avg review length:%.2f.\"%np.mean(review_len))\n",
    "print(\"std review length:%.2f.\"%np.std(review_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ls26FQLeI1Pd"
   },
   "source": [
    "ans:\n",
    "\n",
    "iv: The number of unique words in the whole dataset is 46830.\n",
    "\n",
    "v: The average review lengths is 644, the standard deviation of review lengths is 284.98."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoExGD0awiFS"
   },
   "source": [
    "vi. Plot the histogram of review lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "qIHyz467OHBz",
    "outputId": "5f9bb4e3-939e-481e-e35a-dae8d34e0845"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'length')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(review_len)\n",
    "plt.title('review lengths distribution')\n",
    "plt.xlabel('length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crNC-CFjwqhO"
   },
   "source": [
    "vii. To represent each text (= data point), there are many ways. In NLP/Deep Learning terminology, this task is called tokenization. It is common to represent text using popularity / rank of words in text. The most common word in the text will be represented as 1, the second most common word will be represented as 2, etc. Tokenize each text document using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nyZY5yJLWJXe"
   },
   "outputs": [],
   "source": [
    "token = keras.preprocessing.text.Tokenizer(num_words=5000,lower=True,split=' ')\n",
    "token.fit_on_texts(trainset['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vNExIx-kZPXE"
   },
   "outputs": [],
   "source": [
    "train_token = token.texts_to_sequences(trainset['content'])\n",
    "test_token = token.texts_to_sequences(testset['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFpzXNb0w05K"
   },
   "source": [
    "viii. Select a review length L that 70% of the reviews have a length below it. If you feel more adventurous, set the threshold to 90%.\n",
    "\n",
    "ix. Truncate reviews longer than L words and zero-pad reviews shorter than L so that all texts (= data points) are of length L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0aaXDq5qZaAp"
   },
   "outputs": [],
   "source": [
    "tok_len = int(np.quantile(review_len,0.7))\n",
    "train_pad = pad_sequences(train_token,maxlen=tok_len,padding='pre',truncating='post',value=0)\n",
    "test_pad = pad_sequences(test_token,maxlen=tok_len,padding='pre',truncating='post',value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Go1vvbC69lha"
   },
   "outputs": [],
   "source": [
    "train_y = trainset['class'].values\n",
    "train_y[np.where(train_y == -1)] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cjaa3S2GxEyD"
   },
   "source": [
    "\n",
    "(d)\n",
    "\n",
    "i. Train a MLP with three (dense) hidden layers each of which has 50 ReLUs and one output layer with a single sigmoid neuron. Use a dropout rate of 20% for the first layer and 50% for the other layers. Use ADAM optimizer and binary cross entropy loss (which is equivalent to having a softmax in the output). To avoid overfitting, just set the number of epochs as 2. Use a batch size of 10.\n",
    "\n",
    "ii. Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GaEiWfxBRKgB"
   },
   "outputs": [],
   "source": [
    "def MLP_model():\n",
    "  model = keras.Sequential()\n",
    "  #model.add(layers.Input())\n",
    "  model.add(layers.Embedding(5000,32,input_length=tok_len))\n",
    "  model.add(layers.Flatten())\n",
    "  model.add(layers.Dense(50,activation='relu'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Dense(50,activation='relu'))\n",
    "  model.add(layers.Dropout(0.5))\n",
    "  model.add(layers.Dense(50,activation='relu'))\n",
    "  model.add(layers.Dropout(0.5))\n",
    "  model.add(layers.Dense(1,activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy',optimizer='adam')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FyZnGiY86DRF",
    "outputId": "f3e02a04-9aec-45ac-8cd1-dec06a03fac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "140/140 - 1s - loss: 0.6959 - 1s/epoch - 8ms/step\n",
      "Epoch 2/2\n",
      "140/140 - 1s - loss: 0.6418 - 743ms/epoch - 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8b52c220d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP_model()\n",
    "model.fit(train_pad,train_y,epochs=2,batch_size=10,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "RW6WrIwj8l3x"
   },
   "outputs": [],
   "source": [
    "test_y = testset['class'].values\n",
    "test_y[np.where(test_y == -1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZLVVgUEx7QG0",
    "outputId": "eef038f7-1197-4ec0-b0bc-db19d3ab4af1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is:0.91, the testing accuracy is 0.63.\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.predict(train_pad,verbose=0)\n",
    "pred_train = np.where(pred_train<0.5,0,1)\n",
    "train_acc = accuracy_score(train_y,pred_train)\n",
    "\n",
    "pred_test = model.predict(test_pad,verbose=0)\n",
    "pred_test = np.where(pred_test<0.5,0,1).ravel()\n",
    "test_acc = accuracy_score(test_y,pred_test)\n",
    "print('The training accuracy is:%.2f, the testing accuracy is %.2f.'%(train_acc,test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xcdk4ulSJPwc"
   },
   "source": [
    "The training accuracy is:0.91, the testing accuracy is 0.63."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5m9oAI0CxSz9"
   },
   "source": [
    "(e) One-Dimensional Convolutional Neural Network:\n",
    "\n",
    "Although CNNs are mainly used for image data, they can also be applied to text data, as text also has adjacency information. Keras supports one-dimensional convolutions and pooling by the Conv1D and MaxPooling1D classes respectively.\n",
    "\n",
    "\n",
    "i. After the embedding layer, insert a Conv1D layer. This convolutional layer has 32 feature maps , and each of the 32 kernels has size 3, i.e. reads embedded word representations 3 vector elements of the word embedding at a time. The convolutional layer is followed by a 1D max pooling layer with a length and stride of 2 that halves the size of the feature maps from the convolutional layer. The rest of the network is the same as the neural network above.\n",
    "\n",
    "\n",
    "ii. Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Ta0K9tPeJgW1"
   },
   "outputs": [],
   "source": [
    "def CNN1D_model():\n",
    "  model = keras.Sequential()\n",
    "  model.add(layers.Embedding(5000,32,input_length=tok_len))\n",
    "#  model.add(layers.Flatten())\n",
    "  model.add(layers.Conv1D(filters=32,kernel_size=3))\n",
    "  model.add(layers.MaxPooling1D(pool_size=2,strides=2))\n",
    "  model.add(layers.Flatten())\n",
    "  model.add(layers.Dense(50,activation='relu'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Dense(50,activation='relu'))\n",
    "  model.add(layers.Dropout(0.5))\n",
    "  model.add(layers.Dense(50,activation='relu'))\n",
    "  model.add(layers.Dropout(0.5))\n",
    "  model.add(layers.Dense(1,activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy',optimizer='adam')\n",
    "  return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lylTbzZdPMnx",
    "outputId": "0e750ead-d20b-4df6-8731-c4d19b72f1c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "140/140 - 2s - loss: 0.6991 - 2s/epoch - 11ms/step\n",
      "Epoch 2/2\n",
      "140/140 - 1s - loss: 0.6890 - 1s/epoch - 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8b54f05fa0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = CNN1D_model()\n",
    "model2.fit(train_pad,train_y,epochs=2,batch_size=10,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0w4b0L76XxOi",
    "outputId": "f170e047-8704-4982-eee4-14ea7c39a110"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is:0.83, the testing accuracy is 0.58.\n"
     ]
    }
   ],
   "source": [
    "pred_train2 = model2.predict(train_pad,verbose=0)\n",
    "pred_train2 = np.where(pred_train2<0.5,0,1).ravel()\n",
    "train_acc2 = accuracy_score(train_y,pred_train2)\n",
    "\n",
    "pred_test2 = model2.predict(test_pad,verbose=0)\n",
    "pred_test2 = np.where(pred_test2<0.5,0,1).ravel()\n",
    "test_acc2 = accuracy_score(test_y,pred_test2)\n",
    "\n",
    "print('The training accuracy is:%.2f, the testing accuracy is %.2f.'%(train_acc2,test_acc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6akGixtsJSRz"
   },
   "source": [
    "The training accuracy is:0.83, the testing accuracy is 0.58."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McHJ8sOMxcvx"
   },
   "source": [
    "(f) Long Short-Term Memory Recurrent Neural Network:\n",
    "The structure of the LSTM we are going to use is shown in the following figure.\n",
    "\n",
    "\n",
    "i. Each word is represented to LSTM as a vector of 32 elements and the LSTM is followed by a dense layer of 256 ReLUs. Use a dropout rate of 0.2 for both LSTM and the dense layer. Train the model using 10-50 epochs and batch size of 10.\n",
    "\n",
    "\n",
    "ii. Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "j2KWrhMSX0gI"
   },
   "outputs": [],
   "source": [
    "def LSTM_model():\n",
    "  model = keras.Sequential()\n",
    "  model.add(layers.Embedding(5000,32,input_length=tok_len))\n",
    "  model.add(layers.LSTM(256,dropout=0.2))\n",
    "  model.add(layers.Dense(256,activation='relu'))\n",
    "  model.add(layers.Dropout(0.5))\n",
    "  model.add(layers.Dense(1,activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy',optimizer='adam')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q07ZNGA5Q_Tf",
    "outputId": "8a917be0-13f9-4970-e2f8-874eb8e68a1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "140/140 - 45s - loss: 0.6923 - 45s/epoch - 320ms/step\n",
      "Epoch 2/10\n",
      "140/140 - 46s - loss: 0.6356 - 46s/epoch - 327ms/step\n",
      "Epoch 3/10\n",
      "140/140 - 45s - loss: 0.3831 - 45s/epoch - 321ms/step\n",
      "Epoch 4/10\n",
      "140/140 - 45s - loss: 0.1480 - 45s/epoch - 318ms/step\n",
      "Epoch 5/10\n",
      "140/140 - 46s - loss: 0.0415 - 46s/epoch - 328ms/step\n",
      "Epoch 6/10\n",
      "140/140 - 46s - loss: 0.0234 - 46s/epoch - 330ms/step\n",
      "Epoch 7/10\n",
      "140/140 - 49s - loss: 0.0184 - 49s/epoch - 353ms/step\n",
      "Epoch 8/10\n",
      "140/140 - 47s - loss: 0.0209 - 47s/epoch - 336ms/step\n",
      "Epoch 9/10\n",
      "140/140 - 47s - loss: 0.0266 - 47s/epoch - 338ms/step\n",
      "Epoch 10/10\n",
      "140/140 - 47s - loss: 0.0213 - 47s/epoch - 334ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8b55fdefa0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = LSTM_model()\n",
    "model3.fit(train_pad,train_y,epochs=10,batch_size=10,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mO9EO3WZXizb",
    "outputId": "42839508-856b-4d61-a972-7850022c87c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is:0.97, the testing accuracy is 0.69.\n"
     ]
    }
   ],
   "source": [
    "pred_train3 = model3.predict(train_pad,verbose=0)\n",
    "pred_train3 = np.where(pred_train3<0.5,0,1).ravel()\n",
    "train_acc3 = accuracy_score(train_y,pred_train3)\n",
    "\n",
    "pred_test3 = model3.predict(test_pad,verbose=0)\n",
    "pred_test3 = np.where(pred_test3<0.5,0,1).ravel()\n",
    "test_acc3 = accuracy_score(test_y,pred_test3)\n",
    "\n",
    "print('The training accuracy is:%.2f, the testing accuracy is %.2f.'%(train_acc3,test_acc3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--z1mejqJW7P"
   },
   "source": [
    "The training accuracy is:0.97, the testing accuracy is 0.69."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOMDgvJDupJwMkUC7KyWRjf",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1aZvnvmlCU4QuZthYyk3MibXcpTB64otR",
   "name": "zihanqin_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
